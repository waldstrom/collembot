{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colab Gradio Inference\n\n",
    "Download model weights and helper script from your hosting location, then launch an interactive Gradio app that tiles the image,\n",
    " runs YOLO inference on GPU (when available), fuses detections with the graph-cut strategy, and visualises the final collembola\n",
    "count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install minimal runtime dependencies. Comment out packages you already have in your runtime.\n",
    "!pip install -q ultralytics gradio shapely pillow torch matplotlib requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import requests\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Configure your download locations here\n",
    "# ---------------------------------------------------------------------------\n",
    "REMOTE_BASE = 'https://collembot.ch/colab-resources/'  # hosting root with model, helper, and example image\n",
    "MODEL_URL = REMOTE_BASE + 'collembot-2025_12-yolo11x-seg.pt'\n",
    "HELPER_URL = REMOTE_BASE + 'gradio_app.py'\n",
    "EXAMPLE_URL = REMOTE_BASE + 'example.jpg'\n",
    "\n",
    "ROOT = Path('/content')\n",
    "ROOT.mkdir(exist_ok=True)\n",
    "weights_path = ROOT / 'collembot-2025_12-yolo11x-seg.pt'\n",
    "helper_path = ROOT / 'gradio_app.py'\n",
    "example_path = ROOT / 'example.jpg'\n",
    "\n",
    "USE_GPU = torch.cuda.is_available()\n",
    "PREFERRED_DEVICE = 'cuda:0' if USE_GPU else 'cpu'\n",
    "USE_HALF = False\n",
    "BATCH_SIZE = 12 if USE_GPU else 4\n",
    "\n",
    "if USE_GPU:\n",
    "    name = torch.cuda.get_device_name(torch.cuda.current_device())\n",
    "    print(f'\u2705 GPU detected: {name}')\n",
    "else:\n",
    "    print('\u26a0\ufe0f No GPU detected. In Colab, choose GPU via Runtime > Change runtime type for faster inference.')\n",
    "print(f'Using device={PREFERRED_DEVICE}, half={USE_HALF}, batch={BATCH_SIZE}')\n",
    "\n",
    "\n",
    "def download(url: str, dest: Path):\n",
    "    response = requests.get(url, stream=True)\n",
    "    response.raise_for_status()\n",
    "    dest.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(dest, 'wb') as f:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "    print(f'Downloaded {url} -> {dest}')\n",
    "\n",
    "\n",
    "# Download the helper, weights, and example image. They will overwrite existing copies\n",
    "# to keep the notebook reproducible.\n",
    "download(HELPER_URL, helper_path)\n",
    "download(MODEL_URL, weights_path)\n",
    "download(EXAMPLE_URL, example_path)\n",
    "\n",
    "# Import the freshly downloaded helper module\n",
    "spec = importlib.util.spec_from_file_location('gradio_app', helper_path)\n",
    "gradio_app = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(gradio_app)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch the Gradio UI. Sharing is disabled; run locally.\n",
    "gradio_app.launch_app(\n",
    "    model_path=str(weights_path),\n",
    "    share=False,\n",
    "    example_image=str(example_path),\n",
    "    preferred_device=PREFERRED_DEVICE,\n",
    "    predict_batch_size=BATCH_SIZE,\n",
    "    use_half=USE_HALF,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
